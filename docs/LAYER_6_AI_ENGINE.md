# Layer 6: AI Engine

## Overview

The AI Engine is the brain of K??ra???a OS, providing natural language understanding, dialogue management, reasoning, and action execution. It processes user intents, maintains conversation context, and coordinates with other layers to fulfill requests intelligently.

## Architecture

```
?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
???                      LAYER 6: AI ENGINE                                  ???
?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
???                                                                           ???
???  ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????    ???
???  ???               AICoordinator (Central Intelligence)              ???    ???
???  ???  - Active Model: Phi-3 Mini (3.8B params, INT4)                ???    ???
???  ???  - Context Window: 4096 tokens                                  ???    ???
???  ???  - Inference Time: <200ms per query                             ???    ???
???  ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????    ???
???       ???                                                                  ???
???  ????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????         ???
???  ??? NLU Engine    ??? Dialogue    ??? Reasoning   ??? Action       ???         ???
???  ??? (Intent)      ??? Manager     ??? Engine      ??? Executor     ???         ???
???  ????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????         ???
???       ???                ???              ???              ???                  ???
???  ????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
???  ???            Conversation Memory & Context Tracking                 ???
???  ???  Sessions: 5 active | History: 1000 turns | Context: 2048 tokens ???
???  ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
```

## Core Components

### 1. NLU Engine (Natural Language Understanding)

**Purpose**: Convert user utterances into structured intents with extracted entities.

**Intent Classification**:
```typescript
interface Intent {
  type: IntentType;
  confidence: number;      // 0.0-1.0
  entities: Entity[];
  metadata: {
    complexity: 'simple' | 'moderate' | 'complex';
    requiresReasoning: boolean;
    requiredContext: string[];
  };
}

enum IntentType {
  TRANSFER = 'TRANSFER',
  QUERY_BALANCE = 'QUERY_BALANCE',
  NAVIGATE = 'NAVIGATE',
  SEARCH = 'SEARCH',
  CREATE_REMINDER = 'CREATE_REMINDER',
  CONTROL_SYSTEM = 'CONTROL_SYSTEM',
  ASK_QUESTION = 'ASK_QUESTION',
  UNKNOWN = 'UNKNOWN',
}
```

**Entity Extraction**:
```typescript
interface Entity {
  type: EntityType;
  value: string;
  raw: string;           // Original text
  confidence: number;
}

enum EntityType {
  PERSON = 'PERSON',
  AMOUNT = 'AMOUNT',
  LOCATION = 'LOCATION',
  TIME = 'TIME',
  OBJECT = 'OBJECT',
  ACTION = 'ACTION',
}
```

**Implementation** (`simulator-ui/services/intentClassifier.ts`):
```typescript
class IntentClassifier {
  private model: LanguageModel;
  
  async classify(utterance: string, context: Context): Promise<Intent> {
    // 1. Preprocess
    const normalized = this.normalize(utterance);
    const tokens = this.tokenize(normalized);
    
    // 2. Feature extraction
    const features = {
      keywords: this.extractKeywords(tokens),
      entities: await this.extractEntities(normalized),
      sentiment: this.analyzeSentiment(normalized),
      context: context.toVector(),
    };
    
    // 3. Classify intent
    const intentProbs = await this.model.predict(features);
    const topIntent = this.selectTopIntent(intentProbs);
    
    // 4. Validate with confidence threshold
    if (topIntent.confidence < 0.6) {
      return this.requestClarification(utterance);
    }
    
    return topIntent;
  }
  
  private extractEntities(text: string): Entity[] {
    const entities: Entity[] = [];
    
    // Named Entity Recognition (NER)
    // Extract: persons, amounts, locations, times
    
    // Amount extraction (regex + validation)
    const amountPattern = /(\d+(?:\.\d+)?)\s*(KARA|tokens?)/i;
    const amountMatch = text.match(amountPattern);
    if (amountMatch) {
      entities.push({
        type: EntityType.AMOUNT,
        value: parseFloat(amountMatch[1]),
        raw: amountMatch[0],
        confidence: 0.95,
      });
    }
    
    // Person extraction (capitalized words, contact names)
    const personPattern = /(?:to|for)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)/;
    const personMatch = text.match(personPattern);
    if (personMatch) {
      entities.push({
        type: EntityType.PERSON,
        value: personMatch[1],
        raw: personMatch[0],
        confidence: 0.85,
      });
    }
    
    return entities;
  }
}
```

**Integration Points**:
- **??? Layer 7 (Interface)**: Receive voice/text input
- **??? Dialogue Manager**: Pass structured intent
- **??? Layer 5 (Intelligence)**: Scene context for disambiguation

---

### 2. Dialogue Manager

**Purpose**: Track conversation state, fill missing slots, and handle multi-turn dialogues.

**Dialogue State**:
```typescript
interface DialogueState {
  sessionId: string;
  currentIntent: Intent | null;
  slots: Map<string, SlotValue>;    // Collected information
  history: Turn[];                  // Previous turns
  context: ConversationContext;
}

interface SlotValue {
  name: string;
  value: any;
  confidence: number;
  source: 'user' | 'inferred' | 'default';
}

interface Turn {
  speaker: 'user' | 'assistant';
  text: string;
  timestamp: number;
  intent?: Intent;
}
```

**Slot Filling**:
```typescript
class DialogueManager {
  async processIntent(intent: Intent, state: DialogueState): Promise<DialogueAction> {
    // 1. Check if all required slots filled
    const missingSlots = this.getMissingSlots(intent, state);
    
    if (missingSlots.length > 0) {
      // Request missing information
      return {
        type: 'REQUEST_SLOT',
        prompt: this.generateSlotPrompt(missingSlots[0]),
        targetSlot: missingSlots[0],
      };
    }
    
    // 2. Validate slot values
    const validation = this.validateSlots(intent, state.slots);
    if (!validation.valid) {
      return {
        type: 'CLARIFY',
        prompt: validation.clarificationQuestion,
      };
    }
    
    // 3. All slots filled and valid ??? execute
    return {
      type: 'EXECUTE',
      intent,
      slots: state.slots,
    };
  }
  
  private generateSlotPrompt(slot: SlotDefinition): string {
    const prompts = {
      'recipient': "Who would you like to send to?",
      'amount': "How much would you like to send?",
      'destination': "Where would you like to go?",
      'time': "When should I remind you?",
    };
    return prompts[slot.name] || `Please provide ${slot.name}`;
  }
}
```

**Conversation Example**:
```
User: "Send tokens to Alice"
  Intent: TRANSFER { entities: [person: Alice], slots: {recipient: Alice} }
  Missing: amount
  Response: "How much would you like to send to Alice?"

User: "50 KARA"
  Intent: PROVIDE_AMOUNT { entities: [amount: 50] }
  Slots: {recipient: Alice, amount: 50}
  Response: "Sending 50 KARA to Alice. Please confirm."

User: "Confirm"
  Intent: CONFIRM
  Action: Execute transfer
```

**Integration Points**:
- **??? NLU Engine**: Receive intents
- **??? Reasoning Engine**: Complex decision making
- **??? Action Executor**: Final execution
- **??? Memory System**: Retrieve conversation history

---

### 3. Reasoning Engine

**Purpose**: Handle complex queries requiring multi-step reasoning, tool use, and knowledge integration.

**Reasoning Types**:
```typescript
enum ReasoningType {
  DEDUCTIVE = 'DEDUCTIVE',        // If A then B, A is true, therefore B
  INDUCTIVE = 'INDUCTIVE',        // Pattern recognition from examples
  ABDUCTIVE = 'ABDUCTIVE',        // Best explanation for observation
  ANALOGICAL = 'ANALOGICAL',      // Similarity-based reasoning
  CAUSAL = 'CAUSAL',              // Cause-effect relationships
}
```

**Chain-of-Thought Reasoning**:
```typescript
interface ReasoningStep {
  thought: string;
  tool?: string;
  toolInput?: any;
  observation?: string;
}

class ReasoningEngine {
  async reason(query: string, context: Context): Promise<ReasoningResult> {
    const steps: ReasoningStep[] = [];
    let currentQuery = query;
    
    // Iterative reasoning loop (max 5 steps)
    for (let i = 0; i < 5; i++) {
      // 1. Generate next thought
      const thought = await this.generateThought(currentQuery, steps, context);
      
      // 2. Decide if tool use needed
      const toolDecision = this.decideTool(thought);
      
      if (toolDecision.useTool) {
        // 3. Execute tool
        const observation = await this.executeTool(
          toolDecision.tool,
          toolDecision.input
        );
        
        steps.push({
          thought,
          tool: toolDecision.tool,
          toolInput: toolDecision.input,
          observation,
        });
        
        // 4. Check if answer found
        if (this.isAnswerComplete(observation, query)) {
          return {
            answer: observation,
            reasoning: steps,
            confidence: this.calculateConfidence(steps),
          };
        }
        
        currentQuery = this.refineQuery(query, observation);
      } else {
        // Direct answer without tool
        steps.push({ thought });
        return {
          answer: thought,
          reasoning: steps,
          confidence: 0.9,
        };
      }
    }
    
    return {
      answer: "I need more information to answer this.",
      reasoning: steps,
      confidence: 0.3,
    };
  }
}
```

**Available Tools**:
1. **calculator**: Math operations
2. **search**: Web search via API
3. **memory**: Retrieve past information
4. **blockchain**: Query on-chain data
5. **vision**: Analyze camera feed
6. **time**: Current time/date operations

**Example Reasoning**:
```
Query: "Should I bring an umbrella for my commute?"

Step 1:
  Thought: "I need to check the weather forecast"
  Tool: search("weather forecast Paris today")
  Observation: "70% chance of rain, 15??C"

Step 2:
  Thought: "Rain is likely, but let me check user's past behavior"
  Tool: memory("umbrella preferences")
  Observation: "User usually brings umbrella when >50% rain chance"

Step 3:
  Thought: "Based on 70% rain and user preference, recommend umbrella"
  Answer: "Yes, bring an umbrella. 70% chance of rain and you typically 
           carry one when chances exceed 50%."
  Confidence: 0.92
```

**Integration Points**:
- **??? Dialogue Manager**: Complex queries
- **??? Tool Registry (Layer 4)**: Execute tools
- **??? Universal Oracle**: Agentic reasoning chain

---

### 4. Action Executor

**Purpose**: Convert validated intents into concrete system actions.

**Action Types**:
```typescript
enum ActionType {
  BLOCKCHAIN_TX = 'BLOCKCHAIN_TX',
  SYSTEM_CONTROL = 'SYSTEM_CONTROL',
  APP_LAUNCH = 'APP_LAUNCH',
  NAVIGATION = 'NAVIGATION',
  NOTIFICATION = 'NOTIFICATION',
  DATA_QUERY = 'DATA_QUERY',
}

interface Action {
  type: ActionType;
  params: Record<string, any>;
  requiresConfirmation: boolean;
  estimatedDuration: number;  // milliseconds
}
```

**Execution Pipeline**:
```typescript
class ActionExecutor {
  async execute(intent: Intent, slots: Map<string, SlotValue>): Promise<ActionResult> {
    // 1. Create execution plan
    const plan = this.planExecution(intent, slots);
    
    // 2. Check preconditions
    const preconditionCheck = await this.checkPreconditions(plan);
    if (!preconditionCheck.satisfied) {
      return {
        status: 'FAILED',
        error: preconditionCheck.reason,
      };
    }
    
    // 3. Request confirmation if needed
    if (plan.requiresConfirmation) {
      const confirmed = await this.requestUserConfirmation(plan);
      if (!confirmed) {
        return { status: 'CANCELLED' };
      }
    }
    
    // 4. Execute actions
    try {
      const results = [];
      for (const action of plan.actions) {
        const result = await this.executeAction(action);
        results.push(result);
        
        // Stop on first failure
        if (result.status === 'FAILED') {
          await this.rollback(results.slice(0, -1));
          return result;
        }
      }
      
      return {
        status: 'SUCCESS',
        results,
        duration: plan.estimatedDuration,
      };
    } catch (error) {
      return {
        status: 'ERROR',
        error: error.message,
      };
    }
  }
  
  private async executeAction(action: Action): Promise<ActionResult> {
    switch (action.type) {
      case ActionType.BLOCKCHAIN_TX:
        return await this.blockchainService.submitTransaction(action.params);
        
      case ActionType.SYSTEM_CONTROL:
        return await this.systemService.controlHardware(action.params);
        
      case ActionType.APP_LAUNCH:
        return await this.appService.launchApp(action.params.appId);
        
      case ActionType.NAVIGATION:
        return await this.navService.navigate(action.params.destination);
        
      default:
        throw new Error(`Unknown action type: ${action.type}`);
    }
  }
}
```

**Integration Points**:
- **??? Layer 3 (Blockchain)**: Submit transactions
- **??? Layer 7 (Interface)**: Control UI
- **??? Layer 8 (Apps)**: Launch applications
- **??? Layer 1 (Hardware)**: System controls

---

## Model Management

### On-Device Models

**Phi-3 Mini (Primary)**:
- **Parameters**: 3.8B
- **Quantization**: INT4 (960 MB)
- **Context**: 4096 tokens
- **Inference**: 180-250ms per query (GPU)
- **Use Cases**: NLU, dialogue, reasoning

**Whisper Tiny (Speech Recognition)**:
- **Parameters**: 39M
- **Size**: 75 MB
- **Latency**: <100ms for 3s audio
- **Accuracy**: 92% WER (English)

**CLIP (Vision-Language)**:
- **Parameters**: 151M
- **Size**: 600 MB (FP16)
- **Use Cases**: Image captioning, VQA
- **Latency**: 50ms per image

### Model Quantization

```python
# INT8 quantization (2x compression, 5% accuracy loss)
model_int8 = quantize(model_fp32, dtype=torch.qint8)

# INT4 quantization (4x compression, 10% accuracy loss)
model_int4 = quantize(model_fp32, dtype=torch.quint4x2)

# Performance comparison:
# FP32: 3.8 GB, 800ms inference
# INT8: 1.9 GB, 400ms inference
# INT4: 960 MB, 220ms inference
```

### Model Switching

```typescript
class ModelManager {
  private models: Map<string, Model> = new Map();
  private currentModel: Model;
  
  async switchModel(scenario: Scenario): Promise<void> {
    const targetModel = this.selectModel(scenario);
    
    if (targetModel !== this.currentModel) {
      // Warm up new model
      await this.warmupModel(targetModel);
      
      // Swap models
      this.currentModel = targetModel;
      
      console.log(`Switched to ${targetModel.name}`);
    }
  }
  
  private selectModel(scenario: Scenario): Model {
    // Battery < 20% ??? Use smaller model
    if (scenario.batteryLevel < 0.2) {
      return this.models.get('phi-3-mini-int8');
    }
    
    // Complex query ??? Use full model
    if (scenario.complexity === 'high') {
      return this.models.get('phi-3-mini-int4');
    }
    
    // Default
    return this.currentModel;
  }
}
```

---

## Context Management

### Conversation Memory

```typescript
interface ConversationMemory {
  shortTerm: Turn[];          // Last 10 turns
  longTerm: Summary[];        // Compressed history
  userProfile: UserProfile;   // Preferences, patterns
}

interface Summary {
  timestamp: number;
  topics: string[];
  keyPoints: string[];
  entities: Entity[];
}
```

### Context Window Management

```typescript
class ContextManager {
  private maxTokens = 4096;
  
  buildContext(query: string, memory: ConversationMemory): string {
    const contexts: string[] = [];
    let tokenCount = 0;
    
    // 1. System prompt (always included)
    contexts.push(this.systemPrompt);
    tokenCount += this.countTokens(this.systemPrompt);
    
    // 2. User profile
    const profileStr = this.serializeProfile(memory.userProfile);
    contexts.push(profileStr);
    tokenCount += this.countTokens(profileStr);
    
    // 3. Recent conversation (LIFO)
    for (const turn of memory.shortTerm.reverse()) {
      const turnStr = `${turn.speaker}: ${turn.text}`;
      const turnTokens = this.countTokens(turnStr);
      
      if (tokenCount + turnTokens > this.maxTokens - 512) {
        break; // Leave room for query + response
      }
      
      contexts.unshift(turnStr);
      tokenCount += turnTokens;
    }
    
    // 4. Current query
    contexts.push(`user: ${query}`);
    
    return contexts.join('\n');
  }
}
```

---

## Performance Optimization

### Techniques

1. **Model Caching**: Keep model in GPU memory
2. **KV-Cache**: Cache attention keys/values for faster generation
3. **Speculative Decoding**: Generate multiple tokens in parallel
4. **Batching**: Process multiple queries together
5. **Quantization**: INT4/INT8 for 4x speedup

### Benchmarks

```
?????? AI Engine Performance ??????????????????????????????????????????
??? Intent Classification: 15ms          ???
??? Entity Extraction: 8ms               ???
??? Dialogue State Update: 5ms           ???
??? Reasoning (3 steps): 180ms           ???
??? Action Planning: 12ms                ???
??? Total (average): 220ms               ???
????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
```

---

## Future Development

### Phase 1: Multimodal (Q1 2026)
- Vision-language models for scene understanding
- Audio-visual fusion for richer context
- Gesture recognition integration

### Phase 2: Personalization (Q2 2026)
- Fine-tune on user data (with consent)
- Federated learning across devices
- Privacy-preserving personalization

### Phase 3: Advanced Reasoning (Q3 2026)
- Symbolic + neural hybrid reasoning
- Theorem proving for logic queries
- Causal inference

### Phase 4: Multi-Agent (Q4 2026)
- Specialized sub-agents (finance, health, etc.)
- Agent collaboration protocols
- Swarm intelligence

---

## Code References

- `simulator-ui/services/intentClassifier.ts`: Intent classification
- `simulator-ui/services/enhancedOracleAI.ts`: Dialogue management
- `simulator-ui/services/oracleService.ts`: Universal Oracle integration
- `karana-core/src/ai/agentic.rs`: Reasoning engine (Rust)

---

## Summary

Layer 6 provides:
- **NLU**: Convert speech/text to structured intents
- **Dialogue**: Multi-turn conversation management
- **Reasoning**: Complex query decomposition
- **Execution**: Reliable action fulfillment
- **Context**: Memory and personalization

This layer is the cognitive core, enabling natural interaction with K??ra???a OS.
