# Voice AI Example Commands

**Date:** January 12, 2026  
**Status:** Ready to test  

---

## ðŸŽ¤ Navigation Commands

### Basic Navigation
```
"Go to home"
"Navigate to settings"
"Go back"
"Take me to the main screen"
```

**Expected Behavior:**
- Tool: `navigate(destination="home/settings/back")`
- UI navigates to the specified screen
- Feedback: "Navigated to [destination]"

---

## ðŸ“± App Launching

### Launch Applications
```
"Open camera"
"Launch the wallet"
"Start music player"
"Open browser"
"Show me the weather"
"Open maps"
```

**Expected Behavior:**
- Tool: `launch_app(app_name="camera/wallet/music/browser/weather/maps")`
- App opens in spatial container
- Feedback: "âœ“ Launched [app]"

---

## âœ… Task Management

### Create Tasks
```
"Add task to review PR"
"Create task: call John"
"Add reminder to buy groceries"
"Task to finish documentation"
```

**Expected Behavior:**
- Tool: `create_task(task="[description]")`
- Task appears in task list
- Feedback: "âœ“ Created task: '[description]'"

### Undo Tasks
```
"Undo that"
"Undo the last action"
"Cancel that"
```

**Expected Behavior:**
- Tool: `undo()`
- Last action reversed
- Feedback: "âŸ² Undone: [action]"

---

## ðŸŒ¤ï¸ Weather Queries

### Check Weather
```
"What's the weather?"
"Weather in London"
"How's the weather today?"
"Will it rain tomorrow?"
```

**Expected Behavior:**
- Tool: `weather(location="current/London")`
- Weather info displayed
- Feedback: "Weather in [location]: 72Â°F, Sunny"

### Follow-up Questions
```
User: "What's the weather?"
AI: "Currently 68Â°F and sunny"
User: "Should I bring an umbrella?"
AI: "No rain expected. Clear skies all day."
```

---

## ðŸ’° Wallet Operations

### Check Balance
```
"What's my balance?"
"Show wallet balance"
"How many tokens do I have?"
```

**Expected Behavior:**
- Tool: `wallet(action="balance")`
- Balance displayed
- Feedback: "Your balance: 100.0 KARA tokens"

### Send/Receive
```
"Send tokens"
"Receive payment"
"Show receive address"
```

**Expected Behavior:**
- Tool: `wallet(action="send/receive")`
- Appropriate wallet screen shown

---

## ðŸ—£ï¸ Conversational Commands

### Greetings
```
"Hello"
"Hi there"
"Hey Karana"
```

**Expected Response:**
- "Hello! How can I help you?"

### Thanks
```
"Thank you"
"Thanks"
"Appreciate it"
```

**Expected Response:**
- "You're welcome!"

### Help
```
"What can you do?"
"Help me"
"Show commands"
```

**Expected Response:**
- List of capabilities

---

## ðŸŽ¯ Contextual References

### Using "that" / "it"
```
User: "Open camera"
[Camera opens]
User: "Close that"
[Camera closes - context remembers "camera"]
```

### Using Ordinals
```
[UI shows 3 buttons]
User: "Click the first button"
[First button clicked]

User: "Click the third one"
[Third button clicked]
```

### Using Position
```
[UI shows multiple elements]
User: "Click the top button"
User: "Select the one on the left"
User: "Open the right panel"
```

**Expected Behavior:**
- StateContext resolves reference to specific UI element
- Action performed on correct element

---

## ðŸ”— Chained Commands

### Multiple Actions
```
"Open wallet and check my balance"
"Launch camera and take a photo"
"Go to settings and show system info"
```

**Expected Behavior:**
- Multiple tools executed in sequence
- Each action confirmed separately

---

## ðŸ§ª Testing Scenarios

### Scenario 1: Morning Routine
```
1. "Hello" â†’ Greeting response
2. "What's the weather?" â†’ Weather info
3. "Should I bring an umbrella?" â†’ Contextual answer
4. "Open calendar" â†’ Calendar app launches
5. "Add task to call mom" â†’ Task created
```

### Scenario 2: App Management
```
1. "Open camera" â†’ Camera launches
2. "Take a photo" â†’ Photo captured
3. "Close that" â†’ Camera closes (contextual)
4. "Show me the gallery" â†’ Gallery opens
```

### Scenario 3: Corrections
```
1. "Add task to review code" â†’ Task created
2. "Actually, undo that" â†’ Task removed
3. "Add task to review PR instead" â†’ New task created
```

### Scenario 4: Multi-step Query
```
1. "What's my wallet balance?" â†’ Balance shown
2. "Is that enough to buy coffee?" â†’ Oracle reasoning
3. "Send 5 tokens to John" â†’ Transaction initiated
```

---

## ðŸŽ¨ Testing UI Feedback

### Voice Activity Indicators
- ðŸŽ¤ Microphone icon pulses when listening
- ðŸ“Š Waveform shows audio level
- ðŸ’¬ Real-time transcription appears
- âš¡ Tool results show with confidence score
- âœ“ Success feedback with green badge
- âŒ Error feedback with red badge

### Expected Animations
- Waveform animates during speech
- Smooth fade-in for transcription
- Tool result slides in from bottom
- Undo button appears after actions

---

## ðŸ› Edge Cases to Test

### Empty/Invalid Input
```
"[silence]" â†’ No action
"Ummm..." â†’ Uncertain, no action
"Asdfghjkl" â†’ Error: "I didn't understand that"
```

### Ambiguous Commands
```
"Open it" (with no context) â†’ "What would you like to open?"
"Do that thing" â†’ Clarification requested
```

### Long Commands
```
"Open the camera app and then take a photo and after that show me the gallery and finally close everything"
â†’ Should handle gracefully or break into steps
```

### Interruptions
```
[User speaks]
[User interrupts mid-sentence]
â†’ Should reset and start over
```

---

## ðŸ“Š Success Metrics

Track these during testing:

```typescript
{
  totalCommands: number,
  successfulExecutions: number,
  failedExecutions: number,
  avgConfidence: number,
  avgResponseTime: number,
  undoRate: number,
  clarificationsNeeded: number
}
```

### Target Metrics
- Success rate: >95%
- Average confidence: >0.85
- Response time: <500ms
- Undo rate: <5%

---

## ðŸš€ Running the Tests

### 1. Start Backend
```bash
cd karana-core
cargo run --bin voice_server
```

### 2. Start Frontend
```bash
cd simulator-ui
npm run dev
```

### 3. Open Browser
```
http://localhost:5173
```

### 4. Test Voice Commands
1. Click microphone button
2. Grant microphone permission
3. Say command
4. Verify:
   - Transcription appears
   - Tool executes
   - UI updates
   - Feedback shows

---

## ðŸ” Debugging

### Check Backend Logs
```bash
# Should see:
[VOICE] Processing: 'open camera'
[TOOL] Executing: launch_app
[TOOL] âœ“ Success: Launched camera
[WS] Broadcasting tool result
```

### Check Browser Console
```javascript
// Should see:
[Voice] open camera
[Tool] {tool_name: "launch_app", result: "Launched camera", ...}
[WS] âœ“ Connected
```

### Check WebSocket Connection
```bash
# In browser DevTools Network tab:
# Look for ws://localhost:8080 connection
# Status should be "101 Switching Protocols"
```

---

## âœ… Test Checklist

- [ ] WebSocket connects successfully
- [ ] Microphone permission granted
- [ ] Waveform visualizes audio
- [ ] Voice transcription works
- [ ] Navigation commands execute
- [ ] App launching works
- [ ] Task creation works
- [ ] Weather queries work
- [ ] Wallet commands work
- [ ] Conversational responses work
- [ ] Contextual references resolve
- [ ] Undo functionality works
- [ ] Tool execution feedback shows
- [ ] TTS speaks responses (if enabled)
- [ ] UI updates reflect tool results

---

## ðŸŽ¯ Next Steps

After basic testing passes:

1. **Performance Testing**
   - Test with background noise
   - Test with fast speech
   - Test with accents
   - Measure latency

2. **Stress Testing**
   - Rapid fire commands
   - Very long commands
   - Many concurrent users

3. **Integration Testing**
   - Test with real apps
   - Test with real data
   - Test error recovery

4. **User Testing**
   - Get feedback on naturalness
   - Identify missing commands
   - Improve error messages

---

## ðŸ“ Report Issues

If something doesn't work:

1. Check backend logs
2. Check browser console
3. Verify WebSocket connection
4. Check tool registry has tool
5. Verify state context
6. Test with simpler command

Common fixes:
- Restart voice server
- Clear browser cache
- Check microphone permission
- Rebuild frontend
- Check port 8080 available
